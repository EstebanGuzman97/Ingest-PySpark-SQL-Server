{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dependencies.config import Config\n",
    "#from jobs.etl_jobs import EtlJobs\n",
    "import dependencies.spark as spark\n",
    "import dependencies.logging as log\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "import utils.utils as util\n",
    "from pyspark.sql.types import *\n",
    "#import password as p\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = \"SDBUETMSQL06\"\n",
    "database = \"PAS:Test\"\n",
    "table = \"contrato\" #Para probar aqui cambian el nombre de la tabla en cuestion\n",
    "user = 'X_PAST_TERA_UPS'\n",
    "password = \"Neuquen#Sanmartindelosandes11382\"\n",
    "DRIVERCLASS = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "URL = f'jdbc:sqlserver://{server};databaseName={database}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iniciar sesion en Spark\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"test_pas_tsl\") \\\n",
    "    .setMaster(\"yarn\") \\\n",
    "    .set(\"spark.driver.extraClassPath\",'mssql-jdbc-11.2.0.jre8.jar')\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = sqlContext.sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traer tabla mediante jdbc y guardarlo en un df\n",
    "\n",
    "try:\n",
    "    print(\"Generando Sesión con SQL...\")\n",
    "    \n",
    "    query = f\"SELECT * FROM {table}\" #Aplicar filtro con WHERE para pruebas, no traer tabla completa.\n",
    "      \n",
    "    jdbcDF = spark.read.format(\"jdbc\") \\\n",
    "               .option(\"url\", URL) \\\n",
    "               .option(\"user\", user) \\\n",
    "               .option(\"password\", password) \\\n",
    "               .option(\"driver\", DRIVERCLASS) \\\n",
    "               .option(\"query\", query) \\\n",
    "               .load()      \n",
    "      \n",
    "    #jdbcDF.show()\n",
    "    jdbcDF.printSchema()\n",
    "        \n",
    "except ValueError as error:\n",
    "      logging.info(\"Fallo conexión del conector JDBC con la DB.\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generacion de Path (pueden probarlo con un path local)\n",
    "\n",
    "path = \"/user/se35430a/pas_tsl_test\"\n",
    "path_stg = \"raw_data\"\n",
    "name_table = table\n",
    "path_output_stg = os.path.join(path, name_table.lower(), path_stg, \"\")\n",
    "    \n",
    "print(f'Path sin particionar:{path_output_stg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardado y almacenamiento en formato Parquet (puede modificarse y hacerse a csv entre otros formatos)\n",
    "\n",
    "try:\n",
    "    jdbcDF.write.save(path=path_output_stg, format=\"parquet\", mode=\"overwrite\")\n",
    "    \n",
    "    \n",
    "except ValueError as error:\n",
    "    logging.error(\"No se pudo escribir en HDFS.\", error)\n",
    "    logging.info('Escritura en HDFS correcta')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
